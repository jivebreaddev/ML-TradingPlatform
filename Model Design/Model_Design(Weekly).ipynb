{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Loading Training Sets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"Training_Sets/N_X10weekly.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"Training_Sets/N_Y10weekly.pickle\",\"rb\")\n",
    "Y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Train/Test Split </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "print (Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bikal\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Artificial Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from keras import losses\n",
    "model = tf.keras.models.Sequential()\n",
    "from keras import regularizers\n",
    "\n",
    "model.add(tf.keras.layers.Flatten(input_shape=X_train[0].shape))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(20, kernel_regularizer= regularizers.l2(0.55), activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, kernel_regularizer= regularizers.l2(0.6),   activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#  \n",
    "#  kernel_regularizer = regularizers.l2(0.001) ,\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(20, activation = tf.nn.relu))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(512, activation = tf.nn.relu))\n",
    "# model.add(tf.keras.layers.Dense(4, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2, activation = tf.nn.relu))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001, decay = 0.0001),\n",
    "loss = losses.binary_crossentropy,\n",
    " metrics =[sensitivity, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1386 samples, validate on 154 samples\n",
      "Epoch 1/100\n",
      "1386/1386 [==============================] - 1s 596us/step - loss: 19.1927 - sensitivity: 0.5209 - acc: 0.4762 - val_loss: 18.8391 - val_sensitivity: 0.7409 - val_acc: 0.4675\n",
      "Epoch 2/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 18.6101 - sensitivity: 0.4687 - acc: 0.4820 - val_loss: 18.2793 - val_sensitivity: 0.9173 - val_acc: 0.5260\n",
      "Epoch 3/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 18.0531 - sensitivity: 0.4677 - acc: 0.4978 - val_loss: 17.7370 - val_sensitivity: 0.9074 - val_acc: 0.5195\n",
      "Epoch 4/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 17.5119 - sensitivity: 0.4649 - acc: 0.4957 - val_loss: 17.2090 - val_sensitivity: 0.8975 - val_acc: 0.5260\n",
      "Epoch 5/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 16.9922 - sensitivity: 0.4661 - acc: 0.4971 - val_loss: 16.6974 - val_sensitivity: 0.7880 - val_acc: 0.4935\n",
      "Epoch 6/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 16.4814 - sensitivity: 0.4664 - acc: 0.4971 - val_loss: 16.2026 - val_sensitivity: 0.7002 - val_acc: 0.4935\n",
      "Epoch 7/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 15.9959 - sensitivity: 0.4757 - acc: 0.5108 - val_loss: 15.7256 - val_sensitivity: 0.6593 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "1386/1386 [==============================] - 0s 49us/step - loss: 15.5244 - sensitivity: 0.4683 - acc: 0.5043 - val_loss: 15.2651 - val_sensitivity: 0.6056 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 15.0701 - sensitivity: 0.4697 - acc: 0.5036 - val_loss: 14.8203 - val_sensitivity: 0.5797 - val_acc: 0.4935\n",
      "Epoch 10/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 14.6277 - sensitivity: 0.4711 - acc: 0.5101 - val_loss: 14.3893 - val_sensitivity: 0.4899 - val_acc: 0.4805\n",
      "Epoch 11/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 14.1986 - sensitivity: 0.4681 - acc: 0.5202 - val_loss: 13.9725 - val_sensitivity: 0.4670 - val_acc: 0.4870\n",
      "Epoch 12/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 13.7801 - sensitivity: 0.4775 - acc: 0.5281 - val_loss: 13.5689 - val_sensitivity: 0.4433 - val_acc: 0.4935\n",
      "Epoch 13/100\n",
      "1386/1386 [==============================] - 0s 49us/step - loss: 13.3828 - sensitivity: 0.4868 - acc: 0.5260 - val_loss: 13.1784 - val_sensitivity: 0.4433 - val_acc: 0.5065\n",
      "Epoch 14/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 12.9945 - sensitivity: 0.4587 - acc: 0.5245 - val_loss: 12.7993 - val_sensitivity: 0.3746 - val_acc: 0.4740\n",
      "Epoch 15/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 12.6206 - sensitivity: 0.4630 - acc: 0.5245 - val_loss: 12.4323 - val_sensitivity: 0.3608 - val_acc: 0.4675\n",
      "Epoch 16/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 12.2548 - sensitivity: 0.4648 - acc: 0.5245 - val_loss: 12.0756 - val_sensitivity: 0.3478 - val_acc: 0.4675\n",
      "Epoch 17/100\n",
      "1386/1386 [==============================] - 0s 51us/step - loss: 11.9084 - sensitivity: 0.4489 - acc: 0.5267 - val_loss: 11.7301 - val_sensitivity: 0.3478 - val_acc: 0.4740\n",
      "Epoch 18/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 11.5601 - sensitivity: 0.4491 - acc: 0.5152 - val_loss: 11.3949 - val_sensitivity: 0.3348 - val_acc: 0.4740\n",
      "Epoch 19/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 11.2250 - sensitivity: 0.4642 - acc: 0.5368 - val_loss: 11.0691 - val_sensitivity: 0.3348 - val_acc: 0.4740\n",
      "Epoch 20/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 10.9068 - sensitivity: 0.4475 - acc: 0.5245 - val_loss: 10.7536 - val_sensitivity: 0.2851 - val_acc: 0.4481\n",
      "Epoch 21/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 10.5881 - sensitivity: 0.4573 - acc: 0.5361 - val_loss: 10.4467 - val_sensitivity: 0.2950 - val_acc: 0.4545\n",
      "Epoch 22/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 10.2925 - sensitivity: 0.4582 - acc: 0.5404 - val_loss: 10.1499 - val_sensitivity: 0.2809 - val_acc: 0.4416\n",
      "Epoch 23/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 9.9983 - sensitivity: 0.4401 - acc: 0.5368 - val_loss: 9.8620 - val_sensitivity: 0.2809 - val_acc: 0.4416\n",
      "Epoch 24/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 9.7176 - sensitivity: 0.4460 - acc: 0.5267 - val_loss: 9.5820 - val_sensitivity: 0.2939 - val_acc: 0.4481\n",
      "Epoch 25/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 9.4371 - sensitivity: 0.4457 - acc: 0.5339 - val_loss: 9.3100 - val_sensitivity: 0.2939 - val_acc: 0.4481\n",
      "Epoch 26/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 9.1682 - sensitivity: 0.4505 - acc: 0.5476 - val_loss: 9.0472 - val_sensitivity: 0.2939 - val_acc: 0.4481\n",
      "Epoch 27/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 8.9072 - sensitivity: 0.4383 - acc: 0.5404 - val_loss: 8.7928 - val_sensitivity: 0.2840 - val_acc: 0.4351\n",
      "Epoch 28/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 8.6594 - sensitivity: 0.4302 - acc: 0.5260 - val_loss: 8.5451 - val_sensitivity: 0.2970 - val_acc: 0.4351\n",
      "Epoch 29/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 8.4123 - sensitivity: 0.4373 - acc: 0.5491 - val_loss: 8.3050 - val_sensitivity: 0.2970 - val_acc: 0.4351\n",
      "Epoch 30/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 8.1773 - sensitivity: 0.4159 - acc: 0.5368 - val_loss: 8.0719 - val_sensitivity: 0.2970 - val_acc: 0.4416\n",
      "Epoch 31/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 7.9407 - sensitivity: 0.4231 - acc: 0.5505 - val_loss: 7.8446 - val_sensitivity: 0.2840 - val_acc: 0.4545\n",
      "Epoch 32/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 7.7179 - sensitivity: 0.4195 - acc: 0.5418 - val_loss: 7.6245 - val_sensitivity: 0.2840 - val_acc: 0.4481\n",
      "Epoch 33/100\n",
      "1386/1386 [==============================] - 0s 51us/step - loss: 7.5006 - sensitivity: 0.4125 - acc: 0.5426 - val_loss: 7.4108 - val_sensitivity: 0.2840 - val_acc: 0.4545\n",
      "Epoch 34/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 7.2876 - sensitivity: 0.4295 - acc: 0.5534 - val_loss: 7.2035 - val_sensitivity: 0.2840 - val_acc: 0.4481\n",
      "Epoch 35/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 7.0883 - sensitivity: 0.3991 - acc: 0.5375 - val_loss: 7.0020 - val_sensitivity: 0.2741 - val_acc: 0.4481\n",
      "Epoch 36/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 6.8886 - sensitivity: 0.4303 - acc: 0.5411 - val_loss: 6.8083 - val_sensitivity: 0.2741 - val_acc: 0.4416\n",
      "Epoch 37/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 6.6974 - sensitivity: 0.4230 - acc: 0.5512 - val_loss: 6.6180 - val_sensitivity: 0.2741 - val_acc: 0.4416\n",
      "Epoch 38/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 6.5064 - sensitivity: 0.4163 - acc: 0.5599 - val_loss: 6.4336 - val_sensitivity: 0.2741 - val_acc: 0.4416\n",
      "Epoch 39/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 6.3263 - sensitivity: 0.4173 - acc: 0.5462 - val_loss: 6.2547 - val_sensitivity: 0.2741 - val_acc: 0.4481\n",
      "Epoch 40/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 6.1506 - sensitivity: 0.3991 - acc: 0.5505 - val_loss: 6.0817 - val_sensitivity: 0.2741 - val_acc: 0.4416\n",
      "Epoch 41/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 5.9706 - sensitivity: 0.4245 - acc: 0.5722 - val_loss: 5.9142 - val_sensitivity: 0.2741 - val_acc: 0.4351\n",
      "Epoch 42/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 5.8101 - sensitivity: 0.4256 - acc: 0.5519 - val_loss: 5.7499 - val_sensitivity: 0.2772 - val_acc: 0.4351\n",
      "Epoch 43/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 5.6446 - sensitivity: 0.4304 - acc: 0.5599 - val_loss: 5.5902 - val_sensitivity: 0.2772 - val_acc: 0.4286\n",
      "Epoch 44/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 5.4888 - sensitivity: 0.4238 - acc: 0.5548 - val_loss: 5.4356 - val_sensitivity: 0.2642 - val_acc: 0.4351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 5.3354 - sensitivity: 0.4123 - acc: 0.5455 - val_loss: 5.2863 - val_sensitivity: 0.2642 - val_acc: 0.4286\n",
      "Epoch 46/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 5.1920 - sensitivity: 0.4245 - acc: 0.5570 - val_loss: 5.1407 - val_sensitivity: 0.2642 - val_acc: 0.4416\n",
      "Epoch 47/100\n",
      "1386/1386 [==============================] - 0s 49us/step - loss: 5.0433 - sensitivity: 0.4359 - acc: 0.5570 - val_loss: 5.0010 - val_sensitivity: 0.2642 - val_acc: 0.4286\n",
      "Epoch 48/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 4.9041 - sensitivity: 0.4172 - acc: 0.5664 - val_loss: 4.8647 - val_sensitivity: 0.2772 - val_acc: 0.4156\n",
      "Epoch 49/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 4.7680 - sensitivity: 0.4237 - acc: 0.5628 - val_loss: 4.7303 - val_sensitivity: 0.2772 - val_acc: 0.4221\n",
      "Epoch 50/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 4.6325 - sensitivity: 0.4109 - acc: 0.5620 - val_loss: 4.6017 - val_sensitivity: 0.2772 - val_acc: 0.4286\n",
      "Epoch 51/100\n",
      "1386/1386 [==============================] - 0s 51us/step - loss: 4.5081 - sensitivity: 0.4148 - acc: 0.5693 - val_loss: 4.4758 - val_sensitivity: 0.2772 - val_acc: 0.4351\n",
      "Epoch 52/100\n",
      "1386/1386 [==============================] - 0s 51us/step - loss: 4.3859 - sensitivity: 0.4139 - acc: 0.5556 - val_loss: 4.3530 - val_sensitivity: 0.2772 - val_acc: 0.4416\n",
      "Epoch 53/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 4.2624 - sensitivity: 0.4108 - acc: 0.5548 - val_loss: 4.2361 - val_sensitivity: 0.2543 - val_acc: 0.4286\n",
      "Epoch 54/100\n",
      "1386/1386 [==============================] - 0s 49us/step - loss: 4.1466 - sensitivity: 0.4218 - acc: 0.5707 - val_loss: 4.1217 - val_sensitivity: 0.2741 - val_acc: 0.4416\n",
      "Epoch 55/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 4.0318 - sensitivity: 0.4160 - acc: 0.5570 - val_loss: 4.0109 - val_sensitivity: 0.2642 - val_acc: 0.4351\n",
      "Epoch 56/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 3.9226 - sensitivity: 0.4063 - acc: 0.5577 - val_loss: 3.9028 - val_sensitivity: 0.2780 - val_acc: 0.4481\n",
      "Epoch 57/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 3.8164 - sensitivity: 0.4257 - acc: 0.5570 - val_loss: 3.8003 - val_sensitivity: 0.2978 - val_acc: 0.4416\n",
      "Epoch 58/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 3.7096 - sensitivity: 0.4320 - acc: 0.5620 - val_loss: 3.6986 - val_sensitivity: 0.2978 - val_acc: 0.4416\n",
      "Epoch 59/100\n",
      "1386/1386 [==============================] - 0s 50us/step - loss: 3.6141 - sensitivity: 0.4313 - acc: 0.5628 - val_loss: 3.5994 - val_sensitivity: 0.2978 - val_acc: 0.4416\n",
      "Epoch 60/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 3.5156 - sensitivity: 0.4266 - acc: 0.5620 - val_loss: 3.5040 - val_sensitivity: 0.2840 - val_acc: 0.4286\n",
      "Epoch 61/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 3.4184 - sensitivity: 0.4329 - acc: 0.5758 - val_loss: 3.4094 - val_sensitivity: 0.2978 - val_acc: 0.4416\n",
      "Epoch 62/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 3.3272 - sensitivity: 0.4104 - acc: 0.5642 - val_loss: 3.3193 - val_sensitivity: 0.3108 - val_acc: 0.4416\n",
      "Epoch 63/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 3.2345 - sensitivity: 0.4063 - acc: 0.5584 - val_loss: 3.2316 - val_sensitivity: 0.2840 - val_acc: 0.4286\n",
      "Epoch 64/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 3.1518 - sensitivity: 0.4293 - acc: 0.5786 - val_loss: 3.1484 - val_sensitivity: 0.2970 - val_acc: 0.4416\n",
      "Epoch 65/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 3.0652 - sensitivity: 0.4378 - acc: 0.5729 - val_loss: 3.0672 - val_sensitivity: 0.3108 - val_acc: 0.4351\n",
      "Epoch 66/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.9881 - sensitivity: 0.4308 - acc: 0.5642 - val_loss: 2.9876 - val_sensitivity: 0.3238 - val_acc: 0.4416\n",
      "Epoch 67/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 2.9039 - sensitivity: 0.4237 - acc: 0.5765 - val_loss: 2.9098 - val_sensitivity: 0.3238 - val_acc: 0.4416\n",
      "Epoch 68/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 2.8322 - sensitivity: 0.3972 - acc: 0.5657 - val_loss: 2.8351 - val_sensitivity: 0.3139 - val_acc: 0.4610\n",
      "Epoch 69/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.7587 - sensitivity: 0.4270 - acc: 0.5801 - val_loss: 2.7612 - val_sensitivity: 0.3139 - val_acc: 0.4675\n",
      "Epoch 70/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 2.6887 - sensitivity: 0.4479 - acc: 0.5823 - val_loss: 2.6914 - val_sensitivity: 0.3001 - val_acc: 0.4675\n",
      "Epoch 71/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.6095 - sensitivity: 0.4470 - acc: 0.5844 - val_loss: 2.6237 - val_sensitivity: 0.3229 - val_acc: 0.4545\n",
      "Epoch 72/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 2.5524 - sensitivity: 0.4215 - acc: 0.5693 - val_loss: 2.5575 - val_sensitivity: 0.3229 - val_acc: 0.4545\n",
      "Epoch 73/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 2.4832 - sensitivity: 0.4445 - acc: 0.5671 - val_loss: 2.4940 - val_sensitivity: 0.2862 - val_acc: 0.4545\n",
      "Epoch 74/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.4220 - sensitivity: 0.4287 - acc: 0.5714 - val_loss: 2.4304 - val_sensitivity: 0.3001 - val_acc: 0.4675\n",
      "Epoch 75/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.3600 - sensitivity: 0.4297 - acc: 0.5823 - val_loss: 2.3684 - val_sensitivity: 0.2862 - val_acc: 0.4610\n",
      "Epoch 76/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.2996 - sensitivity: 0.4314 - acc: 0.5794 - val_loss: 2.3092 - val_sensitivity: 0.2862 - val_acc: 0.4740\n",
      "Epoch 77/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 2.2370 - sensitivity: 0.4599 - acc: 0.5960 - val_loss: 2.2554 - val_sensitivity: 0.2862 - val_acc: 0.4675\n",
      "Epoch 78/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 2.1856 - sensitivity: 0.4287 - acc: 0.5758 - val_loss: 2.2012 - val_sensitivity: 0.2862 - val_acc: 0.4610\n",
      "Epoch 79/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 2.1323 - sensitivity: 0.4268 - acc: 0.5808 - val_loss: 2.1494 - val_sensitivity: 0.3122 - val_acc: 0.4740\n",
      "Epoch 80/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 2.0766 - sensitivity: 0.4439 - acc: 0.5830 - val_loss: 2.1000 - val_sensitivity: 0.2992 - val_acc: 0.4481\n",
      "Epoch 81/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 2.0274 - sensitivity: 0.4530 - acc: 0.5859 - val_loss: 2.0470 - val_sensitivity: 0.3390 - val_acc: 0.4675\n",
      "Epoch 82/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.9795 - sensitivity: 0.4346 - acc: 0.5823 - val_loss: 1.9965 - val_sensitivity: 0.3113 - val_acc: 0.4675\n",
      "Epoch 83/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.9288 - sensitivity: 0.4541 - acc: 0.5902 - val_loss: 1.9499 - val_sensitivity: 0.3243 - val_acc: 0.4740\n",
      "Epoch 84/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.8894 - sensitivity: 0.4283 - acc: 0.5801 - val_loss: 1.9030 - val_sensitivity: 0.3113 - val_acc: 0.4740\n",
      "Epoch 85/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.8378 - sensitivity: 0.4543 - acc: 0.5830 - val_loss: 1.8608 - val_sensitivity: 0.3122 - val_acc: 0.4675\n",
      "Epoch 86/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.7954 - sensitivity: 0.4636 - acc: 0.5974 - val_loss: 1.8172 - val_sensitivity: 0.2983 - val_acc: 0.4740\n",
      "Epoch 87/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 1.7616 - sensitivity: 0.4536 - acc: 0.5887 - val_loss: 1.7754 - val_sensitivity: 0.2983 - val_acc: 0.4870\n",
      "Epoch 88/100\n",
      "1386/1386 [==============================] - 0s 48us/step - loss: 1.7130 - sensitivity: 0.4628 - acc: 0.5960 - val_loss: 1.7371 - val_sensitivity: 0.3252 - val_acc: 0.4935\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.6800 - sensitivity: 0.4603 - acc: 0.5909 - val_loss: 1.6991 - val_sensitivity: 0.3122 - val_acc: 0.4870\n",
      "Epoch 90/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.6406 - sensitivity: 0.4534 - acc: 0.5815 - val_loss: 1.6634 - val_sensitivity: 0.2992 - val_acc: 0.4675\n",
      "Epoch 91/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.6068 - sensitivity: 0.4568 - acc: 0.5945 - val_loss: 1.6278 - val_sensitivity: 0.2992 - val_acc: 0.4870\n",
      "Epoch 92/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.5660 - sensitivity: 0.4484 - acc: 0.5859 - val_loss: 1.5927 - val_sensitivity: 0.2981 - val_acc: 0.4805\n",
      "Epoch 93/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.5387 - sensitivity: 0.4551 - acc: 0.5981 - val_loss: 1.5584 - val_sensitivity: 0.2981 - val_acc: 0.4870\n",
      "Epoch 94/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.4986 - sensitivity: 0.4634 - acc: 0.5974 - val_loss: 1.5268 - val_sensitivity: 0.2592 - val_acc: 0.4610\n",
      "Epoch 95/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.4696 - sensitivity: 0.4636 - acc: 0.6068 - val_loss: 1.4974 - val_sensitivity: 0.2851 - val_acc: 0.4805\n",
      "Epoch 96/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.4417 - sensitivity: 0.4711 - acc: 0.5996 - val_loss: 1.4692 - val_sensitivity: 0.2992 - val_acc: 0.4805\n",
      "Epoch 97/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.4125 - sensitivity: 0.4588 - acc: 0.5866 - val_loss: 1.4407 - val_sensitivity: 0.2862 - val_acc: 0.4805\n",
      "Epoch 98/100\n",
      "1386/1386 [==============================] - 0s 47us/step - loss: 1.3841 - sensitivity: 0.4716 - acc: 0.5974 - val_loss: 1.4140 - val_sensitivity: 0.3122 - val_acc: 0.4870\n",
      "Epoch 99/100\n",
      "1386/1386 [==============================] - 0s 45us/step - loss: 1.3560 - sensitivity: 0.4735 - acc: 0.5967 - val_loss: 1.3879 - val_sensitivity: 0.2851 - val_acc: 0.4675\n",
      "Epoch 100/100\n",
      "1386/1386 [==============================] - 0s 46us/step - loss: 1.3273 - sensitivity: 0.4732 - acc: 0.6075 - val_loss: 1.3635 - val_sensitivity: 0.2981 - val_acc: 0.4740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x248e7fcfbe0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 26us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3634620301135174, 0.2981137950699051, 0.4740259744130172]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57086337]\n",
      " [0.48277313]\n",
      " [0.30948868]\n",
      " [0.4751724 ]\n",
      " [0.36198717]\n",
      " [0.468186  ]\n",
      " [0.36117578]\n",
      " [0.36647013]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.62239456]\n",
      " [0.48277313]\n",
      " [0.63872296]\n",
      " [0.5645997 ]\n",
      " [0.48277313]\n",
      " [0.51534545]\n",
      " [0.48277313]\n",
      " [0.67355365]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.37234777]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.3301122 ]\n",
      " [0.4171414 ]\n",
      " [0.664682  ]\n",
      " [0.48277313]\n",
      " [0.5707239 ]\n",
      " [0.54603136]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.35335284]\n",
      " [0.48277313]\n",
      " [0.6576695 ]\n",
      " [0.46882442]\n",
      " [0.43642724]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.81568533]\n",
      " [0.35193586]\n",
      " [0.63877815]\n",
      " [0.63441026]\n",
      " [0.47521988]\n",
      " [0.48277313]\n",
      " [0.5134417 ]\n",
      " [0.48277313]\n",
      " [0.5316906 ]\n",
      " [0.49744955]\n",
      " [0.48277313]\n",
      " [0.35558805]\n",
      " [0.5563802 ]\n",
      " [0.411315  ]\n",
      " [0.48277313]\n",
      " [0.5508251 ]\n",
      " [0.55349153]\n",
      " [0.50067747]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.48543364]\n",
      " [0.53360766]\n",
      " [0.48277313]\n",
      " [0.29576275]\n",
      " [0.53583443]\n",
      " [0.58224946]\n",
      " [0.63302225]\n",
      " [0.6838537 ]\n",
      " [0.62845224]\n",
      " [0.28640723]\n",
      " [0.6563583 ]\n",
      " [0.48277313]\n",
      " [0.46106535]\n",
      " [0.48277313]\n",
      " [0.6040821 ]\n",
      " [0.5030305 ]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.6329269 ]\n",
      " [0.48277313]\n",
      " [0.27165893]\n",
      " [0.70695436]\n",
      " [0.5196276 ]\n",
      " [0.48277313]\n",
      " [0.55790454]\n",
      " [0.40533233]\n",
      " [0.48277313]\n",
      " [0.65233904]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.44222125]\n",
      " [0.53443444]\n",
      " [0.48277313]\n",
      " [0.48920193]\n",
      " [0.48277313]\n",
      " [0.4928158 ]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.5196837 ]\n",
      " [0.25491682]\n",
      " [0.35499942]\n",
      " [0.52558905]\n",
      " [0.60302514]\n",
      " [0.45344818]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.46654657]\n",
      " [0.48277313]\n",
      " [0.4338313 ]\n",
      " [0.48277313]\n",
      " [0.32248387]\n",
      " [0.66160655]\n",
      " [0.5431322 ]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.3811472 ]\n",
      " [0.5054821 ]\n",
      " [0.48277313]\n",
      " [0.62915725]\n",
      " [0.27166688]\n",
      " [0.48277313]\n",
      " [0.38629887]\n",
      " [0.44801474]\n",
      " [0.7802436 ]\n",
      " [0.6646024 ]\n",
      " [0.48277313]\n",
      " [0.2714839 ]\n",
      " [0.7083421 ]\n",
      " [0.64628994]\n",
      " [0.48277313]\n",
      " [0.41359752]\n",
      " [0.36113864]\n",
      " [0.4941307 ]\n",
      " [0.48277313]\n",
      " [0.34650868]\n",
      " [0.48277313]\n",
      " [0.34791464]\n",
      " [0.60457027]\n",
      " [0.544966  ]\n",
      " [0.4939054 ]\n",
      " [0.7340661 ]\n",
      " [0.38353392]\n",
      " [0.48277313]\n",
      " [0.33652273]\n",
      " [0.48277313]\n",
      " [0.48277313]\n",
      " [0.46692955]\n",
      " [0.6292759 ]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('WeeklyANNFeb.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b> K nearest Neigbor </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bikal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.551948051948052\n",
      "2 0.5194805194805194\n",
      "3 0.5454545454545454\n",
      "4 0.525974025974026\n",
      "5 0.538961038961039\n",
      "6 0.5194805194805194\n",
      "7 0.512987012987013\n",
      "8 0.4935064935064935\n",
      "9 0.538961038961039\n",
      "10 0.538961038961039\n",
      "11 0.551948051948052\n",
      "12 0.5714285714285714\n",
      "13 0.5714285714285714\n",
      "14 0.5324675324675324\n",
      "15 0.551948051948052\n",
      "16 0.525974025974026\n",
      "17 0.5324675324675324\n",
      "18 0.512987012987013\n",
      "19 0.5194805194805194\n",
      "20 0.5194805194805194\n",
      "21 0.5064935064935064\n",
      "22 0.5\n",
      "23 0.512987012987013\n",
      "24 0.525974025974026\n",
      "25 0.551948051948052\n",
      "26 0.5324675324675324\n",
      "27 0.538961038961039\n",
      "28 0.5324675324675324\n",
      "29 0.538961038961039\n",
      "30 0.5454545454545454\n",
      "31 0.538961038961039\n",
      "32 0.538961038961039\n",
      "33 0.5454545454545454\n",
      "34 0.512987012987013\n",
      "35 0.525974025974026\n",
      "36 0.5454545454545454\n",
      "37 0.5194805194805194\n",
      "38 0.538961038961039\n",
      "39 0.538961038961039\n",
      "40 0.551948051948052\n",
      "41 0.512987012987013\n",
      "42 0.5714285714285714\n",
      "43 0.5064935064935064\n",
      "44 0.5454545454545454\n",
      "45 0.5324675324675324\n",
      "46 0.5584415584415584\n",
      "47 0.5454545454545454\n",
      "48 0.5714285714285714\n",
      "49 0.5324675324675324\n",
      "50 0.564935064935065\n",
      "51 0.525974025974026\n",
      "52 0.5454545454545454\n",
      "53 0.5194805194805194\n",
      "54 0.5584415584415584\n",
      "55 0.5064935064935064\n",
      "56 0.512987012987013\n",
      "57 0.5064935064935064\n",
      "58 0.5194805194805194\n",
      "59 0.5\n",
      "60 0.5194805194805194\n",
      "61 0.5194805194805194\n",
      "62 0.525974025974026\n",
      "63 0.538961038961039\n",
      "64 0.512987012987013\n",
      "65 0.5324675324675324\n",
      "66 0.5454545454545454\n",
      "67 0.5194805194805194\n",
      "68 0.5454545454545454\n",
      "69 0.564935064935065\n",
      "70 0.5454545454545454\n",
      "71 0.5454545454545454\n",
      "72 0.525974025974026\n",
      "73 0.538961038961039\n",
      "74 0.538961038961039\n",
      "75 0.5454545454545454\n",
      "76 0.5454545454545454\n",
      "77 0.5454545454545454\n",
      "78 0.538961038961039\n",
      "79 0.5324675324675324\n",
      "80 0.5064935064935064\n",
      "81 0.4935064935064935\n",
      "82 0.5\n",
      "83 0.5194805194805194\n",
      "84 0.4935064935064935\n",
      "85 0.5194805194805194\n",
      "86 0.512987012987013\n",
      "87 0.5064935064935064\n",
      "88 0.5064935064935064\n",
      "89 0.487012987012987\n",
      "90 0.512987012987013\n",
      "91 0.512987012987013\n",
      "92 0.5064935064935064\n",
      "93 0.525974025974026\n",
      "94 0.5064935064935064\n",
      "95 0.525974025974026\n",
      "96 0.5064935064935064\n",
      "97 0.525974025974026\n",
      "98 0.512987012987013\n",
      "99 0.512987012987013\n",
      "100 0.5194805194805194\n",
      "101 0.5194805194805194\n",
      "102 0.5\n",
      "103 0.5\n",
      "104 0.4935064935064935\n",
      "105 0.487012987012987\n",
      "106 0.487012987012987\n",
      "107 0.4805194805194805\n",
      "108 0.487012987012987\n",
      "109 0.4935064935064935\n",
      "110 0.4935064935064935\n",
      "111 0.5\n",
      "112 0.5194805194805194\n",
      "113 0.512987012987013\n",
      "114 0.512987012987013\n",
      "115 0.512987012987013\n",
      "116 0.5064935064935064\n",
      "117 0.512987012987013\n",
      "118 0.5064935064935064\n",
      "119 0.5194805194805194\n",
      "120 0.512987012987013\n",
      "121 0.525974025974026\n",
      "122 0.4935064935064935\n",
      "123 0.512987012987013\n",
      "124 0.4935064935064935\n",
      "125 0.4935064935064935\n",
      "126 0.5\n",
      "127 0.525974025974026\n",
      "128 0.5194805194805194\n",
      "129 0.512987012987013\n",
      "130 0.512987012987013\n",
      "131 0.5194805194805194\n",
      "132 0.512987012987013\n",
      "133 0.5\n",
      "134 0.4935064935064935\n",
      "135 0.4935064935064935\n",
      "136 0.487012987012987\n",
      "137 0.487012987012987\n",
      "138 0.487012987012987\n",
      "139 0.5064935064935064\n",
      "140 0.4935064935064935\n",
      "141 0.487012987012987\n",
      "142 0.487012987012987\n",
      "143 0.487012987012987\n",
      "144 0.5064935064935064\n",
      "145 0.5064935064935064\n",
      "146 0.5064935064935064\n",
      "147 0.5\n",
      "148 0.5194805194805194\n",
      "149 0.512987012987013\n",
      "150 0.512987012987013\n",
      "151 0.5064935064935064\n",
      "152 0.525974025974026\n",
      "153 0.5194805194805194\n",
      "154 0.525974025974026\n",
      "155 0.5064935064935064\n",
      "156 0.5064935064935064\n",
      "157 0.5064935064935064\n",
      "158 0.512987012987013\n",
      "159 0.5064935064935064\n",
      "160 0.512987012987013\n",
      "161 0.5064935064935064\n",
      "162 0.512987012987013\n",
      "163 0.5064935064935064\n",
      "164 0.512987012987013\n",
      "165 0.5064935064935064\n",
      "166 0.5064935064935064\n",
      "167 0.5194805194805194\n",
      "168 0.5194805194805194\n",
      "169 0.512987012987013\n",
      "170 0.512987012987013\n",
      "171 0.512987012987013\n",
      "172 0.525974025974026\n",
      "173 0.5194805194805194\n",
      "174 0.525974025974026\n",
      "175 0.5194805194805194\n",
      "176 0.5324675324675324\n",
      "177 0.5194805194805194\n",
      "178 0.525974025974026\n",
      "179 0.512987012987013\n",
      "180 0.512987012987013\n",
      "181 0.5064935064935064\n",
      "182 0.5064935064935064\n",
      "183 0.4935064935064935\n",
      "184 0.5\n",
      "185 0.5064935064935064\n",
      "186 0.512987012987013\n",
      "187 0.512987012987013\n",
      "188 0.5194805194805194\n",
      "189 0.525974025974026\n",
      "190 0.512987012987013\n",
      "191 0.512987012987013\n",
      "192 0.512987012987013\n",
      "193 0.5064935064935064\n",
      "194 0.5064935064935064\n",
      "195 0.5194805194805194\n",
      "196 0.5064935064935064\n",
      "197 0.5\n",
      "198 0.512987012987013\n",
      "199 0.5064935064935064\n",
      "200 0.5\n",
      "201 0.5064935064935064\n",
      "202 0.5064935064935064\n",
      "203 0.4935064935064935\n",
      "204 0.487012987012987\n",
      "205 0.4935064935064935\n",
      "206 0.487012987012987\n",
      "207 0.5\n",
      "208 0.5064935064935064\n",
      "209 0.4935064935064935\n",
      "210 0.5\n",
      "211 0.4935064935064935\n",
      "212 0.5\n",
      "213 0.5\n",
      "214 0.5\n",
      "215 0.5\n",
      "216 0.5064935064935064\n",
      "217 0.5064935064935064\n",
      "218 0.5\n",
      "219 0.5064935064935064\n",
      "220 0.5064935064935064\n",
      "221 0.5064935064935064\n",
      "222 0.512987012987013\n",
      "223 0.512987012987013\n",
      "224 0.5064935064935064\n",
      "225 0.512987012987013\n",
      "226 0.5\n",
      "227 0.512987012987013\n",
      "228 0.5064935064935064\n",
      "229 0.512987012987013\n",
      "230 0.5064935064935064\n",
      "231 0.5064935064935064\n",
      "232 0.5064935064935064\n",
      "233 0.512987012987013\n",
      "234 0.5\n",
      "235 0.5\n",
      "236 0.5064935064935064\n",
      "237 0.5064935064935064\n",
      "238 0.5064935064935064\n",
      "239 0.5\n",
      "240 0.5\n",
      "241 0.4935064935064935\n",
      "242 0.4935064935064935\n",
      "243 0.4935064935064935\n",
      "244 0.487012987012987\n",
      "245 0.4805194805194805\n",
      "246 0.487012987012987\n",
      "247 0.487012987012987\n",
      "248 0.4935064935064935\n",
      "249 0.4935064935064935\n",
      "250 0.5\n",
      "251 0.4935064935064935\n",
      "252 0.4935064935064935\n",
      "253 0.4805194805194805\n",
      "254 0.4935064935064935\n",
      "255 0.4935064935064935\n",
      "256 0.5\n",
      "257 0.4935064935064935\n",
      "258 0.5\n",
      "259 0.512987012987013\n",
      "260 0.512987012987013\n",
      "261 0.5194805194805194\n",
      "262 0.5324675324675324\n",
      "263 0.525974025974026\n",
      "264 0.525974025974026\n",
      "265 0.525974025974026\n",
      "266 0.525974025974026\n",
      "267 0.5324675324675324\n",
      "268 0.525974025974026\n",
      "269 0.5324675324675324\n",
      "270 0.525974025974026\n",
      "271 0.5324675324675324\n",
      "272 0.5194805194805194\n",
      "273 0.5324675324675324\n",
      "274 0.525974025974026\n",
      "275 0.525974025974026\n",
      "276 0.5194805194805194\n",
      "277 0.525974025974026\n",
      "278 0.512987012987013\n",
      "279 0.512987012987013\n",
      "280 0.512987012987013\n",
      "281 0.5194805194805194\n",
      "282 0.5194805194805194\n",
      "283 0.5194805194805194\n",
      "284 0.5194805194805194\n",
      "285 0.5194805194805194\n",
      "286 0.5194805194805194\n",
      "287 0.5064935064935064\n",
      "288 0.5194805194805194\n",
      "289 0.512987012987013\n",
      "290 0.512987012987013\n",
      "291 0.512987012987013\n",
      "292 0.4935064935064935\n",
      "293 0.512987012987013\n",
      "294 0.512987012987013\n",
      "295 0.5194805194805194\n",
      "296 0.512987012987013\n",
      "297 0.5064935064935064\n",
      "298 0.5064935064935064\n",
      "299 0.5064935064935064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "for i in range(1,300):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "\n",
    "    dataset_size = len(X_train)\n",
    "    TwoDim_dataset = X_train.reshape(dataset_size,-1)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fitting the model\n",
    "    knn.fit(TwoDim_dataset, Y_train)\n",
    "\n",
    "\n",
    "    # predict the response\n",
    "    dataset_size = len(X_test)\n",
    "    TwoDim_dataset = X_test.reshape(dataset_size,-1)\n",
    "    pred = knn.predict(TwoDim_dataset)\n",
    "    accu = knn.score(TwoDim_dataset, Y_test)\n",
    "    accuracy = []\n",
    "    for u,j in zip(pred, Y_test):\n",
    "        if u == j:\n",
    "            accuracy.append(1)\n",
    "        else:\n",
    "            accuracy.append(0)\n",
    "\n",
    "    accurate_predictions = [x for x in accuracy if x == 1]\n",
    "    acc = (len(accurate_predictions) / len(accuracy))\n",
    "\n",
    "    print(i, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Saving the KNN Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"KNN-ModelWeekly63.pickle\", \"wb\")\n",
    "pickle.dump(knn, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.39922515,\n",
       "        0.41153838, 0.40758761, 0.40745592, 0.36630208, 0.45269222],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.55678258,\n",
       "        0.16054829, 0.00598181, 0.05977971, 0.15200467, 0.79843759]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
