{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Loading Training Sets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"Training_Sets/N_X10weekly.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"Training_Sets/N_Y10Weekly.pickle\",\"rb\")\n",
    "Y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Train/Test Split </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1386, 2, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bikal\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Artificial Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from keras import losses\n",
    "model = tf.keras.models.Sequential()\n",
    "from keras import regularizers\n",
    "\n",
    "model.add(tf.keras.layers.Flatten(input_shape=X_train[0].shape))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(20, kernel_regularizer= regularizers.l2(0.55), activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, kernel_regularizer= regularizers.l2(0.6),   activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#  \n",
    "#  kernel_regularizer = regularizers.l2(0.001) ,\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(20, activation = tf.nn.relu))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(512, activation = tf.nn.relu))\n",
    "# model.add(tf.keras.layers.Dense(4, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2, activation = tf.nn.relu))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001, decay = 0.0001),\n",
    "loss = losses.binary_crossentropy,\n",
    " metrics =[sensitivity, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1386 samples, validate on 154 samples\n",
      "Epoch 1/100\n",
      "1386/1386 [==============================] - 1s 815us/step - loss: 19.2661 - sensitivity: 0.5468 - acc: 0.5289 - val_loss: 18.7878 - val_sensitivity: 0.6784 - val_acc: 0.4870\n",
      "Epoch 2/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 18.6740 - sensitivity: 0.5437 - acc: 0.5368 - val_loss: 18.2307 - val_sensitivity: 0.6402 - val_acc: 0.4805\n",
      "Epoch 3/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 18.1195 - sensitivity: 0.5410 - acc: 0.5339 - val_loss: 17.6929 - val_sensitivity: 0.5770 - val_acc: 0.5065\n",
      "Epoch 4/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 17.5819 - sensitivity: 0.5355 - acc: 0.5397 - val_loss: 17.1732 - val_sensitivity: 0.5640 - val_acc: 0.5130\n",
      "Epoch 5/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 17.0715 - sensitivity: 0.5170 - acc: 0.5289 - val_loss: 16.6692 - val_sensitivity: 0.5269 - val_acc: 0.5260\n",
      "Epoch 6/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 16.5631 - sensitivity: 0.4980 - acc: 0.5253 - val_loss: 16.1830 - val_sensitivity: 0.5269 - val_acc: 0.5455\n",
      "Epoch 7/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 16.0691 - sensitivity: 0.5156 - acc: 0.5375 - val_loss: 15.7134 - val_sensitivity: 0.5147 - val_acc: 0.5325\n",
      "Epoch 8/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 15.5939 - sensitivity: 0.5104 - acc: 0.5426 - val_loss: 15.2596 - val_sensitivity: 0.5277 - val_acc: 0.5455\n",
      "Epoch 9/100\n",
      "1386/1386 [==============================] - 0s 57us/step - loss: 15.1450 - sensitivity: 0.5065 - acc: 0.5390 - val_loss: 14.8196 - val_sensitivity: 0.5268 - val_acc: 0.5649\n",
      "Epoch 10/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 14.7010 - sensitivity: 0.4933 - acc: 0.5418 - val_loss: 14.3939 - val_sensitivity: 0.5138 - val_acc: 0.5584\n",
      "Epoch 11/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 14.2755 - sensitivity: 0.4762 - acc: 0.5325 - val_loss: 13.9803 - val_sensitivity: 0.5138 - val_acc: 0.5584\n",
      "Epoch 12/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 13.8656 - sensitivity: 0.4709 - acc: 0.5325 - val_loss: 13.5796 - val_sensitivity: 0.5138 - val_acc: 0.5584\n",
      "Epoch 13/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 13.4508 - sensitivity: 0.4729 - acc: 0.5339 - val_loss: 13.1914 - val_sensitivity: 0.5260 - val_acc: 0.5649\n",
      "Epoch 14/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 13.0635 - sensitivity: 0.4771 - acc: 0.5455 - val_loss: 12.8130 - val_sensitivity: 0.5008 - val_acc: 0.5584\n",
      "Epoch 15/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 12.6711 - sensitivity: 0.4734 - acc: 0.5512 - val_loss: 12.4461 - val_sensitivity: 0.5130 - val_acc: 0.5649\n",
      "Epoch 16/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 12.3163 - sensitivity: 0.4694 - acc: 0.5491 - val_loss: 12.0877 - val_sensitivity: 0.5251 - val_acc: 0.5649\n",
      "Epoch 17/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 11.9661 - sensitivity: 0.4522 - acc: 0.5426 - val_loss: 11.7400 - val_sensitivity: 0.5121 - val_acc: 0.5584\n",
      "Epoch 18/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 11.6156 - sensitivity: 0.4606 - acc: 0.5483 - val_loss: 11.4024 - val_sensitivity: 0.4983 - val_acc: 0.5519\n",
      "Epoch 19/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 11.2863 - sensitivity: 0.4522 - acc: 0.5469 - val_loss: 11.0747 - val_sensitivity: 0.4983 - val_acc: 0.5584\n",
      "Epoch 20/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 10.9705 - sensitivity: 0.4427 - acc: 0.5455 - val_loss: 10.7581 - val_sensitivity: 0.5121 - val_acc: 0.5649\n",
      "Epoch 21/100\n",
      "1386/1386 [==============================] - 0s 57us/step - loss: 10.6502 - sensitivity: 0.4679 - acc: 0.5541 - val_loss: 10.4498 - val_sensitivity: 0.4983 - val_acc: 0.5714\n",
      "Epoch 22/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 10.3435 - sensitivity: 0.4608 - acc: 0.5556 - val_loss: 10.1486 - val_sensitivity: 0.4722 - val_acc: 0.5714\n",
      "Epoch 23/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 10.0488 - sensitivity: 0.4394 - acc: 0.5462 - val_loss: 9.8608 - val_sensitivity: 0.4706 - val_acc: 0.5779\n",
      "Epoch 24/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 9.7562 - sensitivity: 0.4423 - acc: 0.5491 - val_loss: 9.5796 - val_sensitivity: 0.4324 - val_acc: 0.5519\n",
      "Epoch 25/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 9.4808 - sensitivity: 0.4485 - acc: 0.5592 - val_loss: 9.3068 - val_sensitivity: 0.4203 - val_acc: 0.5519\n",
      "Epoch 26/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 9.2198 - sensitivity: 0.4329 - acc: 0.5541 - val_loss: 9.0431 - val_sensitivity: 0.4203 - val_acc: 0.5519\n",
      "Epoch 27/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 8.9549 - sensitivity: 0.4126 - acc: 0.5469 - val_loss: 8.7871 - val_sensitivity: 0.4203 - val_acc: 0.5519\n",
      "Epoch 28/100\n",
      "1386/1386 [==============================] - 0s 57us/step - loss: 8.6970 - sensitivity: 0.4308 - acc: 0.5556 - val_loss: 8.5391 - val_sensitivity: 0.4081 - val_acc: 0.5519\n",
      "Epoch 29/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 8.4512 - sensitivity: 0.4333 - acc: 0.5556 - val_loss: 8.2974 - val_sensitivity: 0.3820 - val_acc: 0.5325\n",
      "Epoch 30/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 8.2208 - sensitivity: 0.3966 - acc: 0.5476 - val_loss: 8.0629 - val_sensitivity: 0.3681 - val_acc: 0.5455\n",
      "Epoch 31/100\n",
      "1386/1386 [==============================] - 0s 57us/step - loss: 7.9841 - sensitivity: 0.4105 - acc: 0.5584 - val_loss: 7.8361 - val_sensitivity: 0.3681 - val_acc: 0.5455\n",
      "Epoch 32/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 7.7535 - sensitivity: 0.4117 - acc: 0.5541 - val_loss: 7.6159 - val_sensitivity: 0.3561 - val_acc: 0.5390\n",
      "Epoch 33/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 7.5396 - sensitivity: 0.3795 - acc: 0.5411 - val_loss: 7.4018 - val_sensitivity: 0.3561 - val_acc: 0.5455\n",
      "Epoch 34/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 7.3207 - sensitivity: 0.4084 - acc: 0.5613 - val_loss: 7.1928 - val_sensitivity: 0.3561 - val_acc: 0.5519\n",
      "Epoch 35/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 7.1231 - sensitivity: 0.4050 - acc: 0.5563 - val_loss: 6.9894 - val_sensitivity: 0.3318 - val_acc: 0.5390\n",
      "Epoch 36/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 6.9174 - sensitivity: 0.3798 - acc: 0.5455 - val_loss: 6.7931 - val_sensitivity: 0.3318 - val_acc: 0.5455\n",
      "Epoch 37/100\n",
      "1386/1386 [==============================] - 0s 57us/step - loss: 6.7297 - sensitivity: 0.3793 - acc: 0.5498 - val_loss: 6.6036 - val_sensitivity: 0.3318 - val_acc: 0.5519\n",
      "Epoch 38/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 6.5345 - sensitivity: 0.4051 - acc: 0.5664 - val_loss: 6.4188 - val_sensitivity: 0.3302 - val_acc: 0.5455\n",
      "Epoch 39/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 6.3623 - sensitivity: 0.3850 - acc: 0.5563 - val_loss: 6.2382 - val_sensitivity: 0.3179 - val_acc: 0.5519\n",
      "Epoch 40/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 6.1845 - sensitivity: 0.3755 - acc: 0.5418 - val_loss: 6.0632 - val_sensitivity: 0.3448 - val_acc: 0.5779\n",
      "Epoch 41/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 6.0025 - sensitivity: 0.3830 - acc: 0.5642 - val_loss: 5.8938 - val_sensitivity: 0.3448 - val_acc: 0.5844\n",
      "Epoch 42/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 5.8398 - sensitivity: 0.3690 - acc: 0.5606 - val_loss: 5.7299 - val_sensitivity: 0.3448 - val_acc: 0.5779\n",
      "Epoch 43/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 5.6762 - sensitivity: 0.3804 - acc: 0.5678 - val_loss: 5.5702 - val_sensitivity: 0.3448 - val_acc: 0.5844\n",
      "Epoch 44/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 5.5164 - sensitivity: 0.3807 - acc: 0.5736 - val_loss: 5.4146 - val_sensitivity: 0.3448 - val_acc: 0.5844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 5.3641 - sensitivity: 0.3927 - acc: 0.5714 - val_loss: 5.2645 - val_sensitivity: 0.3457 - val_acc: 0.5844\n",
      "Epoch 46/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 5.2119 - sensitivity: 0.3890 - acc: 0.5584 - val_loss: 5.1178 - val_sensitivity: 0.3457 - val_acc: 0.5909\n",
      "Epoch 47/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 5.0741 - sensitivity: 0.3706 - acc: 0.5613 - val_loss: 4.9771 - val_sensitivity: 0.3466 - val_acc: 0.5909\n",
      "Epoch 48/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 4.9368 - sensitivity: 0.3637 - acc: 0.5584 - val_loss: 4.8387 - val_sensitivity: 0.3336 - val_acc: 0.5844\n",
      "Epoch 49/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 4.8051 - sensitivity: 0.3668 - acc: 0.5548 - val_loss: 4.7052 - val_sensitivity: 0.3214 - val_acc: 0.5844\n",
      "Epoch 50/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 4.6631 - sensitivity: 0.3722 - acc: 0.5599 - val_loss: 4.5752 - val_sensitivity: 0.2962 - val_acc: 0.5714\n",
      "Epoch 51/100\n",
      "1386/1386 [==============================] - 0s 59us/step - loss: 4.5306 - sensitivity: 0.3950 - acc: 0.5722 - val_loss: 4.4511 - val_sensitivity: 0.3214 - val_acc: 0.5779\n",
      "Epoch 52/100\n",
      "1386/1386 [==============================] - 0s 58us/step - loss: 4.4130 - sensitivity: 0.3769 - acc: 0.5657 - val_loss: 4.3309 - val_sensitivity: 0.3214 - val_acc: 0.5714\n",
      "Epoch 53/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 4.2882 - sensitivity: 0.3675 - acc: 0.5613 - val_loss: 4.2119 - val_sensitivity: 0.3482 - val_acc: 0.5909\n",
      "Epoch 54/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 4.1740 - sensitivity: 0.3606 - acc: 0.5649 - val_loss: 4.0966 - val_sensitivity: 0.3352 - val_acc: 0.5779\n",
      "Epoch 55/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 4.0629 - sensitivity: 0.3673 - acc: 0.5685 - val_loss: 3.9858 - val_sensitivity: 0.3214 - val_acc: 0.5779\n",
      "Epoch 56/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 3.9510 - sensitivity: 0.3479 - acc: 0.5664 - val_loss: 3.8770 - val_sensitivity: 0.3230 - val_acc: 0.5779\n",
      "Epoch 57/100\n",
      "1386/1386 [==============================] - 0s 58us/step - loss: 3.8454 - sensitivity: 0.3608 - acc: 0.5664 - val_loss: 3.7720 - val_sensitivity: 0.3482 - val_acc: 0.5909\n",
      "Epoch 58/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 3.7407 - sensitivity: 0.3510 - acc: 0.5599 - val_loss: 3.6702 - val_sensitivity: 0.3230 - val_acc: 0.5779\n",
      "Epoch 59/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 3.6404 - sensitivity: 0.3644 - acc: 0.5707 - val_loss: 3.5707 - val_sensitivity: 0.3230 - val_acc: 0.5779\n",
      "Epoch 60/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 3.5449 - sensitivity: 0.3660 - acc: 0.5678 - val_loss: 3.4746 - val_sensitivity: 0.3230 - val_acc: 0.5714\n",
      "Epoch 61/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 3.4474 - sensitivity: 0.3615 - acc: 0.5750 - val_loss: 3.3820 - val_sensitivity: 0.3351 - val_acc: 0.5779\n",
      "Epoch 62/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 3.3528 - sensitivity: 0.3638 - acc: 0.5758 - val_loss: 3.2936 - val_sensitivity: 0.3230 - val_acc: 0.5779\n",
      "Epoch 63/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 3.2658 - sensitivity: 0.3503 - acc: 0.5707 - val_loss: 3.2055 - val_sensitivity: 0.3100 - val_acc: 0.5844\n",
      "Epoch 64/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 3.1804 - sensitivity: 0.3611 - acc: 0.5613 - val_loss: 3.1206 - val_sensitivity: 0.2970 - val_acc: 0.5779\n",
      "Epoch 65/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 3.1006 - sensitivity: 0.3433 - acc: 0.5534 - val_loss: 3.0375 - val_sensitivity: 0.3118 - val_acc: 0.5779\n",
      "Epoch 66/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 3.0161 - sensitivity: 0.3638 - acc: 0.5823 - val_loss: 2.9576 - val_sensitivity: 0.3489 - val_acc: 0.5909\n",
      "Epoch 67/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 2.9398 - sensitivity: 0.3428 - acc: 0.5678 - val_loss: 2.8793 - val_sensitivity: 0.3489 - val_acc: 0.5909\n",
      "Epoch 68/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 2.8617 - sensitivity: 0.3646 - acc: 0.5729 - val_loss: 2.8037 - val_sensitivity: 0.3489 - val_acc: 0.5974\n",
      "Epoch 69/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 2.7921 - sensitivity: 0.3473 - acc: 0.5649 - val_loss: 2.7308 - val_sensitivity: 0.3359 - val_acc: 0.5909\n",
      "Epoch 70/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 2.7112 - sensitivity: 0.3680 - acc: 0.5851 - val_loss: 2.6611 - val_sensitivity: 0.3229 - val_acc: 0.5844\n",
      "Epoch 71/100\n",
      "1386/1386 [==============================] - 0s 51us/step - loss: 2.6431 - sensitivity: 0.3663 - acc: 0.5801 - val_loss: 2.5933 - val_sensitivity: 0.3100 - val_acc: 0.5779\n",
      "Epoch 72/100\n",
      "1386/1386 [==============================] - 0s 52us/step - loss: 2.5782 - sensitivity: 0.3504 - acc: 0.5678 - val_loss: 2.5272 - val_sensitivity: 0.3100 - val_acc: 0.5779\n",
      "Epoch 73/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 2.5112 - sensitivity: 0.3664 - acc: 0.5786 - val_loss: 2.4625 - val_sensitivity: 0.3229 - val_acc: 0.5844\n",
      "Epoch 74/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 2.4489 - sensitivity: 0.3634 - acc: 0.5758 - val_loss: 2.4000 - val_sensitivity: 0.3229 - val_acc: 0.5844\n",
      "Epoch 75/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 2.3861 - sensitivity: 0.3473 - acc: 0.5750 - val_loss: 2.3402 - val_sensitivity: 0.2979 - val_acc: 0.5714\n",
      "Epoch 76/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 2.3244 - sensitivity: 0.3581 - acc: 0.5779 - val_loss: 2.2815 - val_sensitivity: 0.3109 - val_acc: 0.5714\n",
      "Epoch 77/100\n",
      "1386/1386 [==============================] - 0s 60us/step - loss: 2.2659 - sensitivity: 0.3478 - acc: 0.5794 - val_loss: 2.2257 - val_sensitivity: 0.2979 - val_acc: 0.5584\n",
      "Epoch 78/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 2.2136 - sensitivity: 0.3508 - acc: 0.5722 - val_loss: 2.1699 - val_sensitivity: 0.2979 - val_acc: 0.5714\n",
      "Epoch 79/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 2.1559 - sensitivity: 0.3637 - acc: 0.5786 - val_loss: 2.1159 - val_sensitivity: 0.3100 - val_acc: 0.5714\n",
      "Epoch 80/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 2.1045 - sensitivity: 0.3641 - acc: 0.5866 - val_loss: 2.0632 - val_sensitivity: 0.3359 - val_acc: 0.5909\n",
      "Epoch 81/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 2.0510 - sensitivity: 0.3547 - acc: 0.5844 - val_loss: 2.0136 - val_sensitivity: 0.2979 - val_acc: 0.5714\n",
      "Epoch 82/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 2.0016 - sensitivity: 0.3627 - acc: 0.5823 - val_loss: 1.9657 - val_sensitivity: 0.2979 - val_acc: 0.5649\n",
      "Epoch 83/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 1.9569 - sensitivity: 0.3599 - acc: 0.5808 - val_loss: 1.9182 - val_sensitivity: 0.2979 - val_acc: 0.5649\n",
      "Epoch 84/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 1.9122 - sensitivity: 0.3657 - acc: 0.5772 - val_loss: 1.8712 - val_sensitivity: 0.3239 - val_acc: 0.5714\n",
      "Epoch 85/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 1.8647 - sensitivity: 0.3742 - acc: 0.5779 - val_loss: 1.8294 - val_sensitivity: 0.2979 - val_acc: 0.5519\n",
      "Epoch 86/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 1.8250 - sensitivity: 0.3501 - acc: 0.5657 - val_loss: 1.7884 - val_sensitivity: 0.2979 - val_acc: 0.5584\n",
      "Epoch 87/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 1.7794 - sensitivity: 0.3493 - acc: 0.5786 - val_loss: 1.7470 - val_sensitivity: 0.2979 - val_acc: 0.5714\n",
      "Epoch 88/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 1.7398 - sensitivity: 0.3474 - acc: 0.5722 - val_loss: 1.7069 - val_sensitivity: 0.2840 - val_acc: 0.5584\n",
      "Epoch 89/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 1.6990 - sensitivity: 0.3603 - acc: 0.5808 - val_loss: 1.6686 - val_sensitivity: 0.3109 - val_acc: 0.5714\n",
      "Epoch 90/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 1.6620 - sensitivity: 0.3703 - acc: 0.5830 - val_loss: 1.6311 - val_sensitivity: 0.2840 - val_acc: 0.5584\n",
      "Epoch 91/100\n",
      "1386/1386 [==============================] - 0s 51us/step - loss: 1.6233 - sensitivity: 0.3595 - acc: 0.5873 - val_loss: 1.5942 - val_sensitivity: 0.2840 - val_acc: 0.5584\n",
      "Epoch 92/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 1.5872 - sensitivity: 0.3654 - acc: 0.5779 - val_loss: 1.5614 - val_sensitivity: 0.2840 - val_acc: 0.5584\n",
      "Epoch 93/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 1.5555 - sensitivity: 0.3582 - acc: 0.5931 - val_loss: 1.5283 - val_sensitivity: 0.2979 - val_acc: 0.5909\n",
      "Epoch 94/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 1.5223 - sensitivity: 0.3578 - acc: 0.5808 - val_loss: 1.4960 - val_sensitivity: 0.2979 - val_acc: 0.5649\n",
      "Epoch 95/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 1.4915 - sensitivity: 0.3613 - acc: 0.5880 - val_loss: 1.4651 - val_sensitivity: 0.2979 - val_acc: 0.5649\n",
      "Epoch 96/100\n",
      "1386/1386 [==============================] - 0s 54us/step - loss: 1.4558 - sensitivity: 0.3697 - acc: 0.5952 - val_loss: 1.4327 - val_sensitivity: 0.2979 - val_acc: 0.5584\n",
      "Epoch 97/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 1.4300 - sensitivity: 0.3740 - acc: 0.5794 - val_loss: 1.4058 - val_sensitivity: 0.3239 - val_acc: 0.5779\n",
      "Epoch 98/100\n",
      "1386/1386 [==============================] - 0s 56us/step - loss: 1.3987 - sensitivity: 0.3549 - acc: 0.5866 - val_loss: 1.3794 - val_sensitivity: 0.2979 - val_acc: 0.5649\n",
      "Epoch 99/100\n",
      "1386/1386 [==============================] - 0s 55us/step - loss: 1.3736 - sensitivity: 0.3563 - acc: 0.5823 - val_loss: 1.3509 - val_sensitivity: 0.3109 - val_acc: 0.5714\n",
      "Epoch 100/100\n",
      "1386/1386 [==============================] - 0s 53us/step - loss: 1.3447 - sensitivity: 0.3666 - acc: 0.5815 - val_loss: 1.3262 - val_sensitivity: 0.2979 - val_acc: 0.5584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc1bb4b160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('WeeklyANN.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b> K nearest Neigbor </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bikal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5194805194805194\n",
      "2 0.5194805194805194\n",
      "3 0.4675324675324675\n",
      "4 0.5\n",
      "5 0.525974025974026\n",
      "6 0.538961038961039\n",
      "7 0.5\n",
      "8 0.512987012987013\n",
      "9 0.512987012987013\n",
      "10 0.551948051948052\n",
      "11 0.525974025974026\n",
      "12 0.551948051948052\n",
      "13 0.525974025974026\n",
      "14 0.564935064935065\n",
      "15 0.5714285714285714\n",
      "16 0.5194805194805194\n",
      "17 0.5454545454545454\n",
      "18 0.5454545454545454\n",
      "19 0.5454545454545454\n",
      "20 0.538961038961039\n",
      "21 0.5194805194805194\n",
      "22 0.5194805194805194\n",
      "23 0.525974025974026\n",
      "24 0.551948051948052\n",
      "25 0.5454545454545454\n",
      "26 0.5194805194805194\n",
      "27 0.538961038961039\n",
      "28 0.5584415584415584\n",
      "29 0.564935064935065\n",
      "30 0.525974025974026\n",
      "31 0.5584415584415584\n",
      "32 0.5714285714285714\n",
      "33 0.6038961038961039\n",
      "34 0.6038961038961039\n",
      "35 0.6038961038961039\n",
      "36 0.5974025974025974\n",
      "37 0.6233766233766234\n",
      "38 0.6038961038961039\n",
      "39 0.6298701298701299\n",
      "40 0.5844155844155844\n",
      "41 0.5909090909090909\n",
      "42 0.5909090909090909\n",
      "43 0.5909090909090909\n",
      "44 0.6168831168831169\n",
      "45 0.5974025974025974\n",
      "46 0.6038961038961039\n",
      "47 0.564935064935065\n",
      "48 0.577922077922078\n",
      "49 0.5584415584415584\n",
      "50 0.577922077922078\n",
      "51 0.551948051948052\n",
      "52 0.5714285714285714\n",
      "53 0.5714285714285714\n",
      "54 0.564935064935065\n",
      "55 0.577922077922078\n",
      "56 0.5909090909090909\n",
      "57 0.5909090909090909\n",
      "58 0.6103896103896104\n",
      "59 0.577922077922078\n",
      "60 0.5909090909090909\n",
      "61 0.5844155844155844\n",
      "62 0.577922077922078\n",
      "63 0.564935064935065\n",
      "64 0.5974025974025974\n",
      "65 0.5844155844155844\n",
      "66 0.577922077922078\n",
      "67 0.5714285714285714\n",
      "68 0.5909090909090909\n",
      "69 0.577922077922078\n",
      "70 0.5844155844155844\n",
      "71 0.577922077922078\n",
      "72 0.6168831168831169\n",
      "73 0.5909090909090909\n",
      "74 0.6103896103896104\n",
      "75 0.5974025974025974\n",
      "76 0.5909090909090909\n",
      "77 0.577922077922078\n",
      "78 0.5844155844155844\n",
      "79 0.5714285714285714\n",
      "80 0.5714285714285714\n",
      "81 0.5584415584415584\n",
      "82 0.577922077922078\n",
      "83 0.5584415584415584\n",
      "84 0.5844155844155844\n",
      "85 0.564935064935065\n",
      "86 0.5584415584415584\n",
      "87 0.551948051948052\n",
      "88 0.577922077922078\n",
      "89 0.5714285714285714\n",
      "90 0.5974025974025974\n",
      "91 0.5974025974025974\n",
      "92 0.5844155844155844\n",
      "93 0.5714285714285714\n",
      "94 0.5974025974025974\n",
      "95 0.6103896103896104\n",
      "96 0.6038961038961039\n",
      "97 0.6038961038961039\n",
      "98 0.5844155844155844\n",
      "99 0.564935064935065\n",
      "100 0.5844155844155844\n",
      "101 0.5909090909090909\n",
      "102 0.6168831168831169\n",
      "103 0.5909090909090909\n",
      "104 0.6103896103896104\n",
      "105 0.5909090909090909\n",
      "106 0.6038961038961039\n",
      "107 0.5974025974025974\n",
      "108 0.6038961038961039\n",
      "109 0.6168831168831169\n",
      "110 0.6168831168831169\n",
      "111 0.6103896103896104\n",
      "112 0.6103896103896104\n",
      "113 0.6038961038961039\n",
      "114 0.6103896103896104\n",
      "115 0.5974025974025974\n",
      "116 0.6103896103896104\n",
      "117 0.5909090909090909\n",
      "118 0.6168831168831169\n",
      "119 0.577922077922078\n",
      "120 0.6038961038961039\n",
      "121 0.5909090909090909\n",
      "122 0.5974025974025974\n",
      "123 0.5974025974025974\n",
      "124 0.5909090909090909\n",
      "125 0.5909090909090909\n",
      "126 0.5974025974025974\n",
      "127 0.577922077922078\n",
      "128 0.5974025974025974\n",
      "129 0.5714285714285714\n",
      "130 0.5714285714285714\n",
      "131 0.5714285714285714\n",
      "132 0.5714285714285714\n",
      "133 0.577922077922078\n",
      "134 0.5714285714285714\n",
      "135 0.5974025974025974\n",
      "136 0.577922077922078\n",
      "137 0.5909090909090909\n",
      "138 0.577922077922078\n",
      "139 0.5714285714285714\n",
      "140 0.577922077922078\n",
      "141 0.564935064935065\n",
      "142 0.577922077922078\n",
      "143 0.577922077922078\n",
      "144 0.5909090909090909\n",
      "145 0.5844155844155844\n",
      "146 0.5974025974025974\n",
      "147 0.6103896103896104\n",
      "148 0.6103896103896104\n",
      "149 0.6168831168831169\n",
      "150 0.6103896103896104\n",
      "151 0.6038961038961039\n",
      "152 0.6103896103896104\n",
      "153 0.5909090909090909\n",
      "154 0.5844155844155844\n",
      "155 0.5974025974025974\n",
      "156 0.5974025974025974\n",
      "157 0.6103896103896104\n",
      "158 0.5909090909090909\n",
      "159 0.6038961038961039\n",
      "160 0.5909090909090909\n",
      "161 0.5974025974025974\n",
      "162 0.5974025974025974\n",
      "163 0.6038961038961039\n",
      "164 0.6103896103896104\n",
      "165 0.6038961038961039\n",
      "166 0.6038961038961039\n",
      "167 0.6168831168831169\n",
      "168 0.6168831168831169\n",
      "169 0.6168831168831169\n",
      "170 0.6363636363636364\n",
      "171 0.6363636363636364\n",
      "172 0.6233766233766234\n",
      "173 0.6363636363636364\n",
      "174 0.6298701298701299\n",
      "175 0.6298701298701299\n",
      "176 0.6233766233766234\n",
      "177 0.6168831168831169\n",
      "178 0.6168831168831169\n",
      "179 0.6038961038961039\n",
      "180 0.6103896103896104\n",
      "181 0.5974025974025974\n",
      "182 0.6038961038961039\n",
      "183 0.5974025974025974\n",
      "184 0.5974025974025974\n",
      "185 0.5909090909090909\n",
      "186 0.5909090909090909\n",
      "187 0.5974025974025974\n",
      "188 0.6038961038961039\n",
      "189 0.6038961038961039\n",
      "190 0.6038961038961039\n",
      "191 0.6168831168831169\n",
      "192 0.6103896103896104\n",
      "193 0.6168831168831169\n",
      "194 0.6103896103896104\n",
      "195 0.6103896103896104\n",
      "196 0.6103896103896104\n",
      "197 0.6103896103896104\n",
      "198 0.5974025974025974\n",
      "199 0.6103896103896104\n",
      "200 0.5974025974025974\n",
      "201 0.5844155844155844\n",
      "202 0.6038961038961039\n",
      "203 0.6168831168831169\n",
      "204 0.6103896103896104\n",
      "205 0.6103896103896104\n",
      "206 0.6038961038961039\n",
      "207 0.5844155844155844\n",
      "208 0.6038961038961039\n",
      "209 0.5909090909090909\n",
      "210 0.5974025974025974\n",
      "211 0.5844155844155844\n",
      "212 0.5909090909090909\n",
      "213 0.6038961038961039\n",
      "214 0.5974025974025974\n",
      "215 0.5974025974025974\n",
      "216 0.5909090909090909\n",
      "217 0.5844155844155844\n",
      "218 0.5909090909090909\n",
      "219 0.5844155844155844\n",
      "220 0.5909090909090909\n",
      "221 0.5909090909090909\n",
      "222 0.5974025974025974\n",
      "223 0.5909090909090909\n",
      "224 0.5974025974025974\n",
      "225 0.5844155844155844\n",
      "226 0.577922077922078\n",
      "227 0.577922077922078\n",
      "228 0.577922077922078\n",
      "229 0.577922077922078\n",
      "230 0.577922077922078\n",
      "231 0.5714285714285714\n",
      "232 0.5714285714285714\n",
      "233 0.577922077922078\n",
      "234 0.577922077922078\n",
      "235 0.564935064935065\n",
      "236 0.5714285714285714\n",
      "237 0.5844155844155844\n",
      "238 0.577922077922078\n",
      "239 0.5844155844155844\n",
      "240 0.577922077922078\n",
      "241 0.5844155844155844\n",
      "242 0.5974025974025974\n",
      "243 0.5974025974025974\n",
      "244 0.6038961038961039\n",
      "245 0.6038961038961039\n",
      "246 0.6038961038961039\n",
      "247 0.5974025974025974\n",
      "248 0.6038961038961039\n",
      "249 0.5909090909090909\n",
      "250 0.6038961038961039\n",
      "251 0.5974025974025974\n",
      "252 0.5974025974025974\n",
      "253 0.6103896103896104\n",
      "254 0.6103896103896104\n",
      "255 0.5909090909090909\n",
      "256 0.6038961038961039\n",
      "257 0.5974025974025974\n",
      "258 0.6103896103896104\n",
      "259 0.6103896103896104\n",
      "260 0.6168831168831169\n",
      "261 0.6103896103896104\n",
      "262 0.6168831168831169\n",
      "263 0.6038961038961039\n",
      "264 0.6168831168831169\n",
      "265 0.6103896103896104\n",
      "266 0.6038961038961039\n",
      "267 0.6038961038961039\n",
      "268 0.6038961038961039\n",
      "269 0.5909090909090909\n",
      "270 0.5844155844155844\n",
      "271 0.5909090909090909\n",
      "272 0.5909090909090909\n",
      "273 0.5974025974025974\n",
      "274 0.5909090909090909\n",
      "275 0.5909090909090909\n",
      "276 0.5974025974025974\n",
      "277 0.5974025974025974\n",
      "278 0.6038961038961039\n",
      "279 0.5974025974025974\n",
      "280 0.6038961038961039\n",
      "281 0.6168831168831169\n",
      "282 0.6103896103896104\n",
      "283 0.6168831168831169\n",
      "284 0.6168831168831169\n",
      "285 0.6038961038961039\n",
      "286 0.6103896103896104\n",
      "287 0.6103896103896104\n",
      "288 0.6233766233766234\n",
      "289 0.6103896103896104\n",
      "290 0.6168831168831169\n",
      "291 0.6038961038961039\n",
      "292 0.6168831168831169\n",
      "293 0.6168831168831169\n",
      "294 0.6103896103896104\n",
      "295 0.6168831168831169\n",
      "296 0.6103896103896104\n",
      "297 0.6103896103896104\n",
      "298 0.6168831168831169\n",
      "299 0.6038961038961039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "for i in range(1,300):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "\n",
    "    dataset_size = len(X_train)\n",
    "    TwoDim_dataset = X_train.reshape(dataset_size,-1)\n",
    "\n",
    "    # fitting the model\n",
    "    knn.fit(TwoDim_dataset, Y_train)\n",
    "\n",
    "\n",
    "    # predict the response\n",
    "    dataset_size = len(X_test)\n",
    "    TwoDim_dataset = X_test.reshape(dataset_size,-1)\n",
    "    pred = knn.predict(TwoDim_dataset)\n",
    "    accu = knn.score(TwoDim_dataset, Y_test)\n",
    "    accuracy = []\n",
    "    for u,j in zip(pred, Y_test):\n",
    "        if u == j:\n",
    "            accuracy.append(1)\n",
    "        else:\n",
    "            accuracy.append(0)\n",
    "\n",
    "    accurate_predictions = [x for x in accuracy if x == 1]\n",
    "    acc = (len(accurate_predictions) / len(accuracy))\n",
    "\n",
    "    print(i, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 39)\n",
    "\n",
    "dataset_size = len(X_train)\n",
    "TwoDim_dataset = X_train.reshape(dataset_size,-1)\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(TwoDim_dataset, Y_train)\n",
    "\n",
    "\n",
    "# predict the response\n",
    "dataset_size = len(X_test)\n",
    "TwoDim_dataset = X_test.reshape(dataset_size,-1)\n",
    "pred = knn.predict(TwoDim_dataset)\n",
    "accu = knn.score(TwoDim_dataset, Y_test)\n",
    "accuracy = []\n",
    "for u,j in zip(pred, Y_test):\n",
    "    if u == j:\n",
    "        accuracy.append(1)\n",
    "    else:\n",
    "        accuracy.append(0)\n",
    "\n",
    "accurate_predictions = [x for x in accuracy if x == 1]\n",
    "acc = (len(accurate_predictions) / len(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Saving the KNN Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"KNN-ModelWeekly63.pickle\", \"wb\")\n",
    "pickle.dump(knn, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.39922515,\n",
       "        0.41153838, 0.40758761, 0.40745592, 0.36630208, 0.45269222],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.55678258,\n",
       "        0.16054829, 0.00598181, 0.05977971, 0.15200467, 0.79843759]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
